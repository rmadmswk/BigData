{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_text-generation-with-lstm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1ieD741O3rFbJWTt6QRh3I8F1l9OEsm5R","authorship_tag":"ABX9TyN0okDjmFOSgnIR7ss3K7AG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# LSTM으로 텍스트 생성하기\n","\n","## 글자 수준의 LSTM 텍스트 생성 모델 구현\n","\n","이런 아이디어를 케라스로 구현해 보죠. 먼저 언어 모델을 학습하기 위해 많은 텍스트 데이터가 필요합니다. 위키피디아나 반지의 제왕처럼 아주 큰 텍스트 파일이나 텍스트 파일의 묶음을 사용할 수 있습니다. 이 예에서는 19세기 후반 독일의 철학자 니체의 글을 사용하겠습니다(영어로 번역된 글입니다). 학습할 언어 모델은 일반적인 영어 모델이 아니라 니체의 문체와 특정 주제를 따르는 모델일 것입니다.\n"],"metadata":{"id":"jEWrudT0kD1H"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEAn3HB1jvx_","executionInfo":{"status":"ok","timestamp":1651105879045,"user_tz":-540,"elapsed":939,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"aeef9ec4-b1d6-466b-fec8-dd8b3f474a96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n","606208/600901 [==============================] - 0s 1us/step\n","614400/600901 [==============================] - 0s 1us/step\n","말뭉치 크기: 600893\n"]}],"source":["from tensorflow.keras import utils\n","import numpy as np\n","\n","path = utils.get_file(\n","    'nietzche.txt',\n","    origin = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n","text = open(path).read().lower()\n","print('말뭉치 크기:', len(text))"]},{"cell_type":"code","source":["text[:500]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"B7xjg8rvlEsf","executionInfo":{"status":"ok","timestamp":1651105899768,"user_tz":-540,"elapsed":393,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"06b125b9-62bc-40f6-9d8d-d7874dc7e36e"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'preface\\n\\n\\nsupposing that truth is a woman--what then? is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to truth, have been unskilled and unseemly methods for\\nwinning a woman? certainly she has never allowed herself to be won; and\\nat present every kind of dogma stands with sad and discouraged mien--if,\\nindeed, it s'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# 60개 글자로 된 시퀀스를 추출\n","maxlen = 60\n","\n","# 세 글자씩 건너 뛰면서 새로운 시퀀스를 샘플링\n","step = 3\n","\n","sentences = [] # 추출한 시퀀스 담을 리스트\n","next_chars = [] # 타깃(시퀀스 다음 글자)을 담을 리스트\n","\n","for i in range(0, len(text) - maxlen, step):\n","  sentences.append(text[i:i+maxlen])\n","  next_chars.append(text[i+maxlen])\n","print('시퀀스 개수:', len(sentences))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgUQX56YlU-l","executionInfo":{"status":"ok","timestamp":1651106325346,"user_tz":-540,"elapsed":341,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"b44859c7-23ab-4d4a-a538-c7c387ad8ff9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["시퀀스 개수: 200278\n"]}]},{"cell_type":"code","source":["# 말뭉치에서 고유한 글자를 담은 리스트\n","chars = sorted(list(set(text)))\n","print('고유한 글자:', len(chars))\n","\n","# chars 리스트에 있는 글자와 글자의 인덱스를 매핑한 딕셔너리\n","char_indices = dict((char, char.index(char)) for char in chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aorNxbSgm5cu","executionInfo":{"status":"ok","timestamp":1651106531586,"user_tz":-540,"elapsed":5,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"fbff24be-ef2a-4069-9baa-1228ebf5c59a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["고유한 글자: 57\n"]}]},{"cell_type":"code","source":["# 글자를 원-핫 인코딩하여 0과 1의 이진 배열로 변경\n","x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n","y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n","\n","for i, sentence in enumerate(sentences):\n","  for t, char in enumerate(sentence):\n","    x[i,t,char_indices[char]] = 1\n","  y[i,char_indices[next_chars[i]]] = 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQtlUECLnkTN","executionInfo":{"status":"ok","timestamp":1651106753820,"user_tz":-540,"elapsed":3681,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"49e85e60-0ff3-4769-cd1b-7816810a43b5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"code","source":["# 네크워크 구성\n","from tensorflow.keras import layers, models\n","\n","model = models.Sequential()\n","model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n","model.add(layers.Dense(len(chars), activation='softmax'))"],"metadata":{"id":"tPllvy58oZo8","executionInfo":{"status":"ok","timestamp":1651106975641,"user_tz":-540,"elapsed":335,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n","optimizer = optimizers.RMSprop(lr=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMGamXpso70T","executionInfo":{"status":"ok","timestamp":1651107068185,"user_tz":-540,"elapsed":3,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"db5ad94d-7cb7-4988-c52d-f8f86964e6f2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(RMSprop, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"markdown","source":["## 언어 모델 훈련과 샘플링\n","\n","훈련된 모델과 시드로 쓰일 간단한 텍스트가 주어지면 다음과 같이 반복하여 새로운 텍스트를 생성할 수 있습니다.\n","\n","1.\t지금까지 생성된 텍스트를 주입하여 모델에서 다음 글자에 대한 확률 분포를 뽑습니다.\n","2.\t특정 온도로 이 확률 분포의 가중치를 조정합니다.\n","3.\t가중치가 조정된 분포에서 무작위로 새로운 글자를 샘플링합니다.\n","4.\t새로운 글자를 생성된 텍스트의 끝에 추가합니다.\n","\n","다음 코드는 모델에서 나온 원본 확률 분포의 가중치를 조정하고 새로운 글자의 인덱스를 추출합니다(샘플링 함수입니다):\n"],"metadata":{"id":"7Gf-tjfdsU2W"}},{"cell_type":"markdown","source":["밑이 자연상수 e인 지수함수(e^x)의 그래프<br>\n","https://wooono.tistory.com/214\n","\n","Python Numpy.log()-로그 <br>\n","https://www.delftstack.com/ko/api/numpy/python-numpy-log/\n"],"metadata":{"id":"GHGF9CeMtGp-"}},{"cell_type":"code","source":["def sample(preds, temperature=1.0):\n","  preds = np.asarray(preds).astype('float64')\n","  preds = np.log(preds) / temperature\n","  exp_preds = np.exp(preds)\n","  preds = exp_preds / np.sum(exp_preds)\n","  probas = np.random.multinomial(1,preds,1)\n","  return np.argmax(probas)"],"metadata":{"id":"oVXYRxsXpnTy","executionInfo":{"status":"ok","timestamp":1651108521987,"user_tz":-540,"elapsed":2,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["[과제] 텍스트를 생성한 후 temperature에 따라 생성된 text가 어떻게 다른지 비교하세요. "],"metadata":{"id":"HtgHh0HfxHi9"}},{"cell_type":"code","source":["import random\n","import sys\n","\n","random_seed(42)\n","start_index = random.randint(0, len(text) - maxlen - 1)\n"],"metadata":{"id":"tpBAmZx1vKPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IFt0yShczaGe"},"execution_count":null,"outputs":[]}]}